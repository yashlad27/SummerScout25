#!/bin/bash

# Job Tracker - One-Command Startup Script
# Usage: ./start.sh

set -e

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘          ğŸš€ JOB TRACKER - STARTUP SCRIPT                       â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Check if Docker is running
echo "ğŸ“‹ Checking Docker..."
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Docker is not running!"
    echo "   Please start Docker Desktop and try again."
    exit 1
fi
echo "âœ… Docker is running"
echo ""

# Check if we need to build
echo "ğŸ”§ Checking if images need to be built..."
if ! docker images | grep -q "linkedin_job_scrapper-worker"; then
    echo "ğŸ”¨ Building Docker images (this may take a few minutes)..."
    docker-compose build
    echo "âœ… Images built successfully"
else
    echo "âœ… Images already built"
fi
echo ""

# Create database tables if needed
echo "ğŸ—„ï¸  Setting up database..."
docker-compose up -d db redis
sleep 3

# Check if tables exist
TABLES=$(docker exec job_tracker_db psql -U jobtracker -d job_tracker -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='public' AND table_name='jobs';" 2>/dev/null || echo "0")

if [ "$TABLES" = "0" ] || [ -z "$TABLES" ]; then
    echo "ğŸ“¦ Creating database tables..."
    docker-compose run --rm worker python -c "from src.core.models import Job, JobVersion, Watchlist, Alert; from src.core.database import Base, engine; Base.metadata.create_all(engine); print('âœ… Tables created!')"
else
    echo "âœ… Database tables already exist"
fi
echo ""

# Start all services
echo "ğŸš€ Starting all services..."
docker-compose up -d
echo ""

# Wait a moment for services to stabilize
sleep 2

# Show status
echo "ğŸ“Š Service Status:"
docker-compose ps
echo ""

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘                  âœ… JOB TRACKER IS RUNNING!                    â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“§ Email notifications: yashlad727@gmail.com"
echo "â° Schedule: Every 4 hours (12am, 4am, 8am, 12pm, 4pm, 8pm)"
echo "ğŸ¢ Companies tracked: 108"
echo "ğŸ¯ Categories: ML/AI, Cybersecurity, Data Science, Data Engineering"
echo ""
echo "ğŸ“‹ Useful Commands:"
echo "   View logs:          docker-compose logs -f worker"
echo "   View schedule:      docker-compose logs -f beat"
echo "   Export jobs:        docker-compose run --rm worker python export_jobs.py"
echo "   Run scrape now:     docker-compose run --rm worker python -m src.ingest.runner"
echo "   Stop tracker:       docker-compose down"
echo ""
echo "ğŸ‰ All set! Jobs will be scraped automatically every 4 hours."
echo ""
